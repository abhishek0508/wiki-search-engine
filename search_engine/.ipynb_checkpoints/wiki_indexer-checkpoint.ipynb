{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_xml_path = \"/home/abhishek/dev/Semester_3/ire/wiki-search-engine/2019201007/data/wiki_dump.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-3b02390c7677>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-e49a6113c601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# from nltk.stem import PorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mStemmer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from copy import deepcopy\n",
    "import xml.sax.handler\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.stem import PorterStemmer\n",
    "from Stemmer import Stemmer\n",
    "from string import punctuation\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import time\n",
    "import sys\n",
    "import errno\n",
    "import heapq\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-70046366e44a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext_punc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpunctuation\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpunc\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'{'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'['\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m']'\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtext_punc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(list(char for char in punctuation))\n",
    "stemmer = Stemmer('english')\n",
    "text_punc = list(punc for punc in punctuation if punc not in ['{', '}', '=', '[', ']' ])\n",
    "text_punc.append('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateIndex(xml.sax.ContentHandler):\n",
    "    def __init__(self):\n",
    "        self.istitle = False\n",
    "        self.istext = False\n",
    "        self.isfirstid = False\n",
    "        self.isid = False\n",
    "        self.title = \"\"\n",
    "        self.text = \"\"\n",
    "        self.docid = \"\"\n",
    "        self.inverted_index = dict()\n",
    "        self.page_count = 0\n",
    "        self.file_count = 0\n",
    "        self.first = 0\n",
    "        \n",
    "    def title_processing(self,title_string):\n",
    "        title_frequency = dict()\n",
    "        title_string = re.sub('\\\\b[-\\.]\\\\b', '', title_string)\n",
    "        title_string = re.sub('[^A-Za-z0-9\\{\\}\\[\\]\\=]+',' ', title_string)\n",
    "        for each_word in wordpunct_tokenize(title_string):\n",
    "            # each_word = each_word.lower()\n",
    "            if each_word.lower() not in stop_words:\n",
    "                each_word = stemmer.stemWord(each_word.lower())\n",
    "                if each_word not in title_frequency:\n",
    "                    title_frequency[each_word] = 0  \n",
    "                title_frequency[each_word]+= 1\n",
    "\n",
    "        return title_frequency\n",
    "    \n",
    "    def text_processing(self,text_string):\n",
    "        # t = title, b = body, i = infobox, c = category, l = link, r = reference\n",
    "\n",
    "        text_string = re.sub('[^A-Za-z0-9\\{\\}\\[\\]\\=]+',' ', text_string)\n",
    "        text_frequency = dict()\n",
    "\n",
    "        regex_category = re.compile(r'\\[\\[Category(.*?)\\]\\]')\n",
    "        table = str.maketrans(dict.fromkeys('\\{\\}\\=\\[\\]'))\n",
    "\n",
    "        new_text = regex_category.split(text_string)\n",
    "        \n",
    "        if len(new_text) > 1:\n",
    "            for text in new_text[1:]:\n",
    "                text = text.translate(table)\n",
    "                for word in wordpunct_tokenize(text):\n",
    "                    # word = word.lower()\n",
    "                    if word.lower() not in text_frequency:\n",
    "                        text_frequency[word.lower()] = dict(t=0,b=0,i=0,c=0,l=0,r=0)\n",
    "                    text_frequency[word.lower()]['c'] += 1\n",
    "\n",
    "            text_string = new_text[0]\n",
    "\n",
    "        new_text = text_string.split('==External links==')\n",
    "        if len(new_text) > 1:\n",
    "            new_text[1] = new_text[1].translate(table)\n",
    "\n",
    "            for word in wordpunct_tokenize(new_text[1]):\n",
    "                # word = word.lower()\n",
    "                if word.lower() not in text_frequency:\n",
    "                    text_frequency[word.lower()] = dict(t=0,b=0,i=0,c=0,l=0,r=0)\n",
    "                text_frequency[word.lower()]['l'] += 1\n",
    "\n",
    "            text_string = new_text[0]\n",
    "\n",
    "        new_text = text_string.split(\"{{Infobox\")\n",
    "\n",
    "        braces_count = 1\n",
    "        default_tag_type = 'i'\n",
    "\n",
    "        if len(new_text) > 1:\n",
    "            new_text[0] = new_text[0].translate(table)\n",
    "\n",
    "            for word in wordpunct_tokenize(new_text[0]):\n",
    "                # word = word.lower()\n",
    "                if word.lower() not in text_frequency:\n",
    "                    text_frequency[word.lower()] = dict(t=0,b=0,i=0,c=0,l=0,r=0)\n",
    "                text_frequency[word.lower()]['b'] += 1\n",
    "\n",
    "\n",
    "\n",
    "            for word in re.split(r\"[^A-Za-z0-9]+\",new_text[1]):\n",
    "                # word = word.lower()\n",
    "                if \"}}\" in word.lower():\n",
    "                    braces_count -= 1\n",
    "                if \"{{\" in word.lower():\n",
    "                    braces_count += 1\n",
    "                    continue\n",
    "                if braces_count == 0:\n",
    "                    default_tag_type = 'b'\n",
    "\n",
    "                word = word.lower().translate(table)\n",
    "\n",
    "                if word not in text_frequency:\n",
    "                    text_frequency[word] = dict(t=0,b=0,i=0,c=0,l=0,r=0)\n",
    "                text_frequency[word][default_tag_type] += 1\n",
    "\n",
    "\n",
    "        else:\n",
    "            text_string = text_string.translate(table)\n",
    "            for word in wordpunct_tokenize(text_string):\n",
    "                word = word.lower()\n",
    "                if word.lower() not in text_frequency:\n",
    "                    text_frequency[word.lower()] = dict(t=0,b=0,i=0,c=0,l=0,r=0)\n",
    "                text_frequency[word.lower()]['b'] += 1\n",
    "\n",
    "\n",
    "        duplicate_copy = dict()\n",
    "        for term in text_frequency:\n",
    "            stemmed_term = stemmer.stemWord(term)\n",
    "            if stemmed_term not in duplicate_copy:\n",
    "                duplicate_copy[stemmed_term] = text_frequency[term]\n",
    "            else:\n",
    "                for key in duplicate_copy[stemmed_term]:\n",
    "                    duplicate_copy[stemmed_term][key] += text_frequency[term][key]\n",
    "\n",
    "        text_frequency = dict()\n",
    "        for term in duplicate_copy:\n",
    "             if term not in stop_words or term != '':\n",
    "                text_frequency[term] = duplicate_copy[term]\n",
    "\n",
    "        return text_frequency\n",
    "    \n",
    "    def preprocessing(self,title,text):\n",
    "\n",
    "        page_count = self.page_count\n",
    "        title_frequency = self.title_processing(title)\n",
    "        text_frequency = self.text_processing(text)\n",
    "        file_pointer = open(\"DocID_Title_mapping.txt\",'a+')\n",
    "        if self.first == 1:\n",
    "            file_pointer.write('\\n')\n",
    "\n",
    "        if self.first == 0:\n",
    "            self.first = 1\n",
    "\n",
    "        value = str(page_count) + ' '+ title\n",
    "        value = value.encode('utf-8').decode()\n",
    "\n",
    "        for word_title in title_frequency:\n",
    "            if word_title in text_frequency:\n",
    "                text_frequency[word_title]['t'] += title_frequency[word_title]\n",
    "            else:\n",
    "                text_frequency[word_title] = dict(d=page_count,t=title_frequency[word_title],b=0,i=0,c=0,l=0,r=0)\n",
    "\n",
    "        file_pointer.write(value)\n",
    "        file_pointer.close()\n",
    "\n",
    "        for term in text_frequency:\n",
    "            if len(term) < 3 or term.startswith('0'):\n",
    "                continue\n",
    "            text_frequency[term]['d'] = str(page_count)\n",
    "            if term not in self.inverted_index:\n",
    "                self.inverted_index[term] = list()\n",
    "            self.inverted_index[term].append(''.join(tag + str(text_frequency[term][tag]) for tag in text_frequency[term] if text_frequency[term][tag] != 0))\n",
    "\n",
    "\n",
    "        if self.page_count%30000 == 0:\n",
    "            print(\"----------------------------here first ever document-------------------\")\n",
    "            writing_to_file(self.inverted_index,self.file_count,'intermediate')\n",
    "            self.file_count = self.file_count + 1\n",
    "            self.inverted_index = dict()\n",
    "        \n",
    "    def startElement(self,name,attribute):\n",
    "        if name == \"title\":\n",
    "            self.istitle = True\n",
    "            self.title = \"\"\n",
    "        elif name == \"text\":\n",
    "            self.istext = True\n",
    "            self.text = \"\"\n",
    "        elif name == \"page\":\n",
    "            self.isfirstid = True\n",
    "            self.docid = \"\"\n",
    "        elif name == \"id\" and self.isfirstid:\n",
    "            self.isid = True\n",
    "            self.isfirstid = False\n",
    "            \n",
    "    def endElement(self,name):\n",
    "        if name == \"title\":\n",
    "            self.istitle = False\n",
    "        elif name == \"text\":\n",
    "            self.istext = False\n",
    "        elif name == \"id\":\n",
    "            self.isid = False\n",
    "#             self.isfirstid = False ## Changeable Areas\n",
    "        elif name == \"page\":\n",
    "            self.page_count = self.page_count + 1\n",
    "            text = deepcopy(self.text)\n",
    "            title = deepcopy(self.title)\n",
    "            id_page = deepcopy(self.docid)\n",
    "            self.preprocessing(title,text)\n",
    "#             print(\"---------ID-------\")\n",
    "#             print(id1)\n",
    "#             print(\"---------TITLE-------\")\n",
    "#             print(title)\n",
    "#             print(\"---------TEXT--------\")\n",
    "#             print(text)\n",
    "        \n",
    "    def characters(self,content):\n",
    "        if self.istitle:\n",
    "            self.title = self.title + content\n",
    "        elif self.istext:\n",
    "            self.text = self.text + content\n",
    "        elif self.isid:\n",
    "            self.docid = self.docid + content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordpunct_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f5a0e1b9e944>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mIndexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCreateIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mxml_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetContentHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mxml_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_xml_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/snap/jupyter/6/lib/python3.7/xml/sax/expatreader.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cont_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetDocumentLocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExpatLocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mxmlreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIncrementalParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;31m# bpo-30264: Close the source on error to not leak resources:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/snap/jupyter/6/lib/python3.7/xml/sax/xmlreader.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/snap/jupyter/6/lib/python3.7/xml/sax/expatreader.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, data, isFinal)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# document. When feeding chunks, they are not normally final -\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;31m# except when invoked from close.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misFinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexpat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAXParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mErrorString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/conda/feedstock_root/build_artifacts/python_1553725065783/work/Modules/pyexpat.c\u001b[0m in \u001b[0;36mEndElement\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/snap/jupyter/6/lib/python3.7/xml/sax/expatreader.py\u001b[0m in \u001b[0;36mend_element\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mend_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cont_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendElement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_element_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-6bc24b0eea2d>\u001b[0m in \u001b[0;36mendElement\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mid_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;31m#             print(\"---------ID-------\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;31m#             print(id1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-6bc24b0eea2d>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(self, title, text)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mpage_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mtitle_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mtext_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mfile_pointer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DocID_Title_mapping.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'a+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-6bc24b0eea2d>\u001b[0m in \u001b[0;36mtitle_processing\u001b[0;34m(self, title_string)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtitle_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\\\b[-\\.]\\\\b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtitle_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[^A-Za-z0-9\\{\\}\\[\\]\\=]+'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0meach_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordpunct_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0;31m# each_word = each_word.lower()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0meach_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wordpunct_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "xml_parser = xml.sax.make_parser()\n",
    "Indexer = CreateIndex()\n",
    "xml_parser.setContentHandler(Indexer)\n",
    "xml_parser.parse(wiki_xml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\\\': None, '{': None, '}': None, '=': None, '[': None, ']': None}\n"
     ]
    }
   ],
   "source": [
    "# regex_category = re.compile(r'\\[\\[Category(.*?)\\]\\]')\n",
    "x = dict.fromkeys('\\{\\}\\=\\[\\]')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{92: None, 123: None, 125: None, 61: None, 91: None, 93: None}\n"
     ]
    }
   ],
   "source": [
    "table = str.maketrans(x)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "text_frequency = dict()\n",
    "print(text_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from copy import deepcopy\n",
    "import xml.sax.handler\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "# from Stemmer import Stemmer\n",
    "from string import punctuation\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import time\n",
    "import sys\n",
    "import errno\n",
    "import heapq\n",
    "import shutil\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words.update(list(char for char in punctuation))\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "text_punc = list(punc for punc in punctuation if punc not in ['{', '}', '=', '[', ']' ])\n",
    "\n",
    "text_punc.append('\\n')\n",
    "\n",
    "\n",
    "# words_left = ['{', '}', '=', '[', ']' ]\n",
    "\n",
    "def writing_to_file(Inverted_Index, File_count,file_path):\n",
    "\tpath_to_write = os.path.join(file_path,str(File_count) + '.txt')\n",
    "\t#print(\"File\",str(File_count))\n",
    "\tvalue = list()\n",
    "\tfile_pointer = open(path_to_write, 'w+')\n",
    "\tfor term in sorted(Inverted_Index):\n",
    "\t\ttemp = term + ' '\n",
    "\t\ttemp = temp + '|'.join(item for item in Inverted_Index[term])\n",
    "\t\tvalue.append(temp)\n",
    "\tif len(value):\n",
    "\t\tfile_pointer.write('\\n'.join(value).encode('utf-8').decode())\n",
    "\n",
    "\tfile_pointer.close()\n",
    "\n",
    "\n",
    "def main_file_write(words,inverted_index,index_file_path,offset_file_path,offset):\n",
    "\titems_to_write = list()\n",
    "\toffset_list = list()\n",
    "\ttry:\n",
    "\t\tfile_pointer = open(index_file_path, 'a+')\n",
    "\t\tfile_pointer1 = open(offset_file_path, 'a+')\n",
    "\t\tfor word in words:\n",
    "\t\t\toffset_term = word + ' ' + str(offset)\n",
    "\t\t\tword_text = word + ' '\n",
    "\t\t\tword_text = word_text + '|'.join(list(item for item in inverted_index[word]))\n",
    "\t\t\toffset_list.append(offset_term)\n",
    "\t\t\titems_to_write.append(word_text)\n",
    "\t\t\toffset = offset + len(word_text) + 1\n",
    "\n",
    "\t\tif len(offset_list):\n",
    "\t\t\tfile_pointer1.write('\\n'.join(offset_list).encode('utf-8').decode())\n",
    "\t\t\tfile_pointer1.write('\\n')\n",
    "\n",
    "\t\tif len(items_to_write):\n",
    "\t\t\tfile_pointer.write('\\n'.join(items_to_write).encode('utf-8').decode())\n",
    "\t\t\tfile_pointer.write('\\n')\n",
    "\t\t\t\n",
    "\n",
    "\t\tfile_pointer.close()\n",
    "\t\tfile_pointer1.close()\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"Error while opening the Index File. Exiting..\")\n",
    "\tfinally:\n",
    "\t\tfile_pointer.close()\n",
    "\t\tfile_pointer1.close()\n",
    "\n",
    "\treturn offset\n",
    "\n",
    "\n",
    "def Merge_files(file_count,index_path):\n",
    "\n",
    "\tif not os.path.exists(index_path):\n",
    "\t\ttry:\n",
    "\t\t\tos.makedirs(index_path)\n",
    "\t\texcept OSError as e:\n",
    "\t\t\tif e.errno == errno.EEXIST:\n",
    "\t\t\t\traise\n",
    "\n",
    "\tfile_pointer = list()\n",
    "\tend_of_file = list()\n",
    "\tlist_of_words = list()\n",
    "\theap = list()\n",
    "\n",
    "\tfor index in range(file_count):\n",
    "\t\tpath_of_file = os.path.join('intermediate', str(index) + '.txt')\n",
    "\t\tfile_pointer.append(open(path_of_file, 'r'))\n",
    "\t\tlist_of_words.append(file_pointer[index].readline().split(' ', 1))\n",
    "\t\tif list_of_words[index][0] not in heap:\n",
    "\t\t\theapq.heappush(heap,list_of_words[index][0])\n",
    "\t\tend_of_file.append(0)\n",
    "\n",
    "\tindex_file_path = os.path.join(index_path, 'index_file' + '.txt')\n",
    "\toffset_file_path = \"offset_file.txt\"\n",
    "\ttry:\n",
    "\t\tos.remove(index_file_path)\n",
    "\t\tos.remove(offset_file_path)\n",
    "\texcept OSError as e:\n",
    "\t\tif e.errno != errno.ENOENT:\n",
    "\t\t\traise\n",
    "\n",
    "\toffset = 0\n",
    "\tflag = 0\n",
    "\twords = list()\n",
    "\tinverted_index = dict()\n",
    "\twhile heap:\n",
    "\t\ttop_most_word = heapq.heappop(heap)\n",
    "\t\tif top_most_word == \"\":\n",
    "\t\t\tcontinue\n",
    "\t\twords.append(top_most_word)\n",
    "\t\tif top_most_word not in inverted_index:\n",
    "\t\t\tinverted_index[top_most_word] = list()\n",
    "\n",
    "\t\tfor index in range(file_count):\n",
    "\n",
    "\t\t\tif end_of_file[index] == 1:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tif list_of_words[index][0] == top_most_word:\n",
    "\t\t\t\tinverted_index[top_most_word].append(list_of_words[index][1].strip())\n",
    "\t\t\t\tlist_of_words[index] = file_pointer[index].readline().split(' ', 1)\n",
    "\n",
    "\t\t\t\tif list_of_words[index][0] == \"\":\n",
    "\t\t\t\t\tfile_pointer[index].close()\n",
    "\t\t\t\t\tend_of_file[index] = 1\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\tif list_of_words[index][0] not in heap:\n",
    "\t\t\t\t\theapq.heappush(heap,list_of_words[index][0])\n",
    "\n",
    "\t\tif len(words)%100000 == 0:\n",
    "\t\t\toffset = main_file_write(words,inverted_index,index_file_path,offset_file_path,offset)\n",
    "\t\t\tflag = 1\n",
    "\t\t\tinverted_index = dict()\n",
    "\t\t\twords = list()\n",
    "\n",
    "\tif len(words):\n",
    "\t\toffset = main_file_write(words,inverted_index,index_file_path,offset_file_path,offset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CreateIndex(xml.sax.ContentHandler):\n",
    "\n",
    "\tdef __init__(self):\n",
    "\n",
    "\t\tself.istitle = False\n",
    "\t\tself.istext = False\n",
    "\t\tself.isfirstid = False\n",
    "\t\tself.isid = False\n",
    "\t\tself.title = \"\"\n",
    "\t\tself.text = \"\"\n",
    "\t\tself.docid = \"\"\n",
    "\t\tself.inverted_index = dict()\n",
    "\t\tself.page_count = 0\n",
    "\t\tself.file_count = 0\n",
    "\t\tself.first = 0\n",
    "\n",
    "\tdef title_processing(self,title_string):\n",
    "\n",
    "\t\ttitle_frequency = dict()\n",
    "\t\ttitle_string = re.sub('\\\\b[-\\.]\\\\b', '', title_string)\n",
    "\t\ttitle_string = re.sub('[^A-Za-z0-9\\{\\}\\[\\]\\=]+',' ', title_string)\n",
    "\t\tfor each_word in wordpunct_tokenize(title_string):\n",
    "\t\t\t# each_word = each_word.lower()\n",
    "\t\t\tif each_word.lower() not in stop_words:\n",
    "\t\t\t\teach_word = stemmer.stem(each_word.lower())\n",
    "\t\t\t\tif each_word not in title_frequency:\n",
    "\t\t\t\t\ttitle_frequency[each_word] = 0  \n",
    "\t\t\t\ttitle_frequency[each_word]+= 1\n",
    "\n",
    "\t\treturn title_frequency\n",
    "\n",
    "\n",
    "\tdef text_processing(self,text_string):\n",
    "\n",
    "\t\ttext_string = re.sub('[^A-Za-z0-9\\{\\}\\[\\]\\=]+',' ', text_string)\n",
    "\t\ttext_frequency = dict()\n",
    "\n",
    "\t\tregex_category = re.compile(r'\\[\\[Category(.*?)\\]\\]')\n",
    "\t\ttable = str.maketrans(dict.fromkeys('\\{\\}\\=\\[\\]'))\n",
    "\n",
    "\t\tnew_text = regex_category.split(text_string)\n",
    "\n",
    "\t\tif len(new_text) > 1:\n",
    "\t\t\tfor text in new_text[1:]:\n",
    "\t\t\t\ttext = text.translate(table)\n",
    "\t\t\t\tfor word in wordpunct_tokenize(text):\n",
    "\t\t\t\t\t# word = word.lower()\n",
    "\t\t\t\t\tif word.lower() not in text_frequency:\n",
    "\t\t\t\t\t\ttext_frequency[word.lower()] = dict(t=0,b=0,i=0,c=0,l=0,r=0)\n",
    "\t\t\t\t\ttext_frequency[word.lower()]['c'] += 1\n",
    "\n",
    "\t\t\ttext_string = new_text[0]\n",
    "\n",
    "\t\tnew_text = text_string.split('==External links==')\n",
    "\t\tif len(new_text) > 1:\n",
    "\t\t\tnew_text[1] = new_text[1].translate(table)\n",
    "\n",
    "\t\t\tfor word in wordpunct_tokenize(new_text[1]):\n",
    "\t\t\t\t# word = word.lower()\n",
    "\t\t\t\tif word.lower() not in text_frequency:\n",
    "\t\t\t\t\ttext_frequency[word.lower()] = dict(t=0,b=0,i=0,c=0,l=0,r=0)\n",
    "\t\t\t\ttext_frequency[word.lower()]['l'] += 1\n",
    "\n",
    "\t\t\ttext_string = new_text[0]\n",
    "\n",
    "\t\tnew_text = text_string.split(\"{{Infobox\")\n",
    "\n",
    "\t\tbraces_count = 1\n",
    "\t\tdefault_tag_type = 'i'\n",
    "\n",
    "\t\tif len(new_text) > 1:\n",
    "\t\t\tnew_text[0] = new_text[0].translate(table)\n",
    "\n",
    "\t\t\tfor word in wordpunct_tokenize(new_text[0]):\n",
    "\t\t\t\t# word = word.lower()\n",
    "\t\t\t\tif word.lower() not in text_frequency:\n",
    "\t\t\t\t\ttext_frequency[word.lower()] = dict(t=0,b=0,i=0,c=0,l=0,r=0)\n",
    "\t\t\t\ttext_frequency[word.lower()]['b'] += 1\n",
    "\n",
    "\t\t\t\n",
    "\n",
    "\t\t\tfor word in re.split(r\"[^A-Za-z0-9]+\",new_text[1]):\n",
    "\t\t\t\t# word = word.lower()\n",
    "\t\t\t\tif \"}}\" in word.lower():\n",
    "\t\t\t\t\tbraces_count -= 1\n",
    "\t\t\t\tif \"{{\" in word.lower():\n",
    "\t\t\t\t\tbraces_count += 1\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tif braces_count == 0:\n",
    "\t\t\t\t\tdefault_tag_type = 'b'\n",
    "\n",
    "\t\t\t\tword = word.lower().translate(table)\n",
    "\n",
    "\t\t\t\tif word not in text_frequency:\n",
    "\t\t\t\t\ttext_frequency[word] = dict(t=0,b=0,i=0,c=0,l=0,r=0)\n",
    "\t\t\t\ttext_frequency[word][default_tag_type] += 1\n",
    "\n",
    "\n",
    "\t\telse:\n",
    "\t\t\ttext_string = text_string.translate(table)\n",
    "\t\t\tfor word in wordpunct_tokenize(text_string):\n",
    "\t\t\t\tword = word.lower()\n",
    "\t\t\t\tif word.lower() not in text_frequency:\n",
    "\t\t\t\t\ttext_frequency[word.lower()] = dict(t=0,b=0,i=0,c=0,l=0,r=0)\n",
    "\t\t\t\ttext_frequency[word.lower()]['b'] += 1\n",
    "\n",
    "\n",
    "\t\tduplicate_copy = dict()\n",
    "\t\tfor term in text_frequency:\n",
    "\t\t\tstemmed_term = stemmer.stem(term)\n",
    "\t\t\tif stemmed_term not in duplicate_copy:\n",
    "\t\t\t\tduplicate_copy[stemmed_term] = text_frequency[term]\n",
    "\t\t\telse:\n",
    "\t\t\t\tfor key in duplicate_copy[stemmed_term]:\n",
    "\t\t\t\t\tduplicate_copy[stemmed_term][key] += text_frequency[term][key]\n",
    "\n",
    "\t\ttext_frequency = dict()\n",
    "\t\tfor term in duplicate_copy:\n",
    "\t\t\t if term not in stop_words or term != '':\n",
    "\t\t\t \ttext_frequency[term] = duplicate_copy[term]\n",
    "\n",
    "\t\treturn text_frequency\n",
    "\n",
    "\n",
    "\tdef preprocessing(self,title,text):\n",
    "\n",
    "\t\tpage_count = self.page_count\n",
    "\t\ttitle_frequency = self.title_processing(title)\n",
    "\t\ttext_frequency = self.text_processing(text)\n",
    "\t\tfile_pointer = open(\"DocID_Title_mapping.txt\",'a+')\n",
    "\t\tif self.first == 1:\n",
    "\t\t\tfile_pointer.write('\\n')\n",
    "\n",
    "\t\tif self.first == 0:\n",
    "\t\t\tself.first = 1\n",
    "\n",
    "\t\tvalue = str(page_count) + ' '+ title\n",
    "\t\tvalue = value.encode('utf-8').decode()\n",
    "\n",
    "\t\tfor word_title in title_frequency:\n",
    "\t\t\tif word_title in text_frequency:\n",
    "\t\t\t\ttext_frequency[word_title]['t'] += title_frequency[word_title]\n",
    "\t\t\telse:\n",
    "\t\t\t\ttext_frequency[word_title] = dict(d= page_count,t=title_frequency[word_title],b=0,i=0,c=0,l=0,r=0)\n",
    "\n",
    "\t\tfile_pointer.write(value)\n",
    "\t\tfile_pointer.close()\n",
    "\n",
    "\t\tfor term in text_frequency:\n",
    "\t\t\tif len(term) < 3 or term.startswith('0'):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\ttext_frequency[term]['d'] = str(page_count)\n",
    "\t\t\tif term not in self.inverted_index:\n",
    "\t\t\t\tself.inverted_index[term] = list()\n",
    "\t\t\tself.inverted_index[term].append(''.join(tag + str(text_frequency[term][tag]) for tag in text_frequency[term] if text_frequency[term][tag] != 0))\n",
    "\n",
    "\n",
    "\t\tif self.page_count%30000 == 0:\n",
    "\t\t\twriting_to_file(self.inverted_index,self.file_count,'intermediate')\n",
    "\t\t\tself.file_count = self.file_count + 1\n",
    "\t\t\tself.inverted_index = dict()\n",
    "\n",
    "\n",
    "\tdef startElement(self,name,attribute):\n",
    "\n",
    "\t\tif name == \"title\":\n",
    "\t\t\tself.istitle = True\n",
    "\t\t\tself.title = \"\"\n",
    "\t\telif name == \"text\":\n",
    "\t\t\tself.istext = True\n",
    "\t\t\tself.text = \"\"\n",
    "\t\telif name == \"page\":\n",
    "\t\t\tself.isfirstid = True\n",
    "\t\t\tself.docid = \"\"\n",
    "\t\telif name == \"id\" and self.isfirstid:\n",
    "\t\t\tself.id = True\n",
    "\t\t\tself.isfirstid = False\n",
    "\n",
    "\tdef endElement(self,name):\n",
    "\n",
    "\t\tif name == \"title\":\n",
    "\t\t\tself.istitle = False\n",
    "\t\telif name == \"text\":\n",
    "\t\t\tself.istext = False\n",
    "\t\telif name == \"id\":\n",
    "\t\t\tself.isid = False\n",
    "\t\t\t#self.isfirstid = False ## Changeable Areas\n",
    "\t\telif name == \"page\":\n",
    "\t\t\tself.page_count = self.page_count + 1\n",
    "\t\t\ttext = deepcopy(self.text)\n",
    "\t\t\ttitle = deepcopy(self.title)\n",
    "\t\t\tself.preprocessing(title,text)\n",
    "\n",
    "\n",
    "\tdef characters(self, content):\n",
    "\n",
    "\t\tif self.istitle:\n",
    "\t\t\tself.title = self.title + content\n",
    "\t\telif self.istext:\n",
    "\t\t\tself.text = self.text + content\n",
    "\t\telif self.isid:\n",
    "\t\t\tself.docid = self.docid + content\n",
    "\n",
    "def create_offset_files():\n",
    "\tif not os.path.exists('temp_offsets'):\n",
    "\t\tos.mkdir('temp_offsets')\n",
    "\telse:\n",
    "\t\tshutil.rmtree('temp_offsets')\n",
    "\t\tos.mkdir('temp_offsets')\n",
    "\n",
    "\tfile_ptr = None\n",
    "\twith open('offset_file.txt') as offset_file:\n",
    "\t\tfor lineno,line in enumerate(offset_file):\n",
    "\t\t\tif lineno % 1000000 == 0:\n",
    "\t\t\t\tif file_ptr:\n",
    "\t\t\t\t\tfile_ptr = None\n",
    "\t\t\t\tvalue = line.strip().split(' ')[0]\n",
    "\t\t\t\tfile_path = os.path.join('temp_offsets',value + '.txt')\n",
    "\t\t\t\tfile_ptr = open(file_path,\"w\")\n",
    "\t\t\tfile_ptr.write(line)\n",
    "\t\tif file_ptr:\n",
    "\t\t\tfile_ptr.close()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\tsys.setrecursionlimit(1500)\n",
    "\n",
    "\tstart = time.time()\n",
    "\tif not os.path.exists('intermediate'):\n",
    "\t\ttry:\n",
    "\t\t\tos.makedirs('intermediate')\n",
    "\t\texcept OSError as e:\n",
    "\t\t\tif e.errno == errno.EEXIST:\n",
    "\t\t\t\traise\n",
    "\n",
    "\ttry:\n",
    "\t\tos.remove('DocID_Title_mapping.txt')\n",
    "\texcept OSError as e:\n",
    "\t\tpass\n",
    "\n",
    "\txml_parser = xml.sax.make_parser()\n",
    "\n",
    "\tIndexer = CreateIndex()\n",
    "\n",
    "\txml_parser.setContentHandler(Indexer)\n",
    "\n",
    "\txml_parser.parse(sys.argv[1])\n",
    "\n",
    "\tif Indexer.page_count % 30000 > 0:\n",
    "\t\twriting_to_file(Indexer.inverted_index, Indexer.file_count,'intermediate')\n",
    "\t\tIndexer.file_count += 1\n",
    "\n",
    "\tMerge_files(Indexer.file_count,sys.argv[2])\n",
    "\tend = time.time()\n",
    "\tprint(\"Time Taken to build an Inverted Index is : \" + str(end - start) + \" seconds\")\n",
    "\tshutil.rmtree('intermediate')\n",
    "\tcreate_offset_files()\n",
    "\tos.remove(\"offset_file.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
